# evals-example

A small playground for experimenting with LLM evaluation libraries (Ragas, DeepEval, etc.).

## Project layout

```
├── evals_example                  # Stand-alone example scripts
│   └── ragas_aspect_critic.py     # Evaluates LLM outputs with Ragas + AspectCritic
├── pyproject.toml                 # Project metadata and dependencies
└── uv.lock                        # Frozen dependency versions (generated by `uv`)
```

## Quickstart

1.  Install the dependencies (Python ≥ 3.11 is required):

    ```bash
    uv pip install -r pyproject.toml
    ```

2.  Run the AspectCritic example:

    ```bash
    python ./evals_example/ragas_aspect_critic.py
    ```

    The script prints a dictionary containing the French translation of a sample
    sentence ("Hello, how are you?") and the corresponding AspectCritic score.

## Example scripts

Each evaluation script lives directly under `evals_example/` and follows the naming convention `{eval_framework}_{eval_metric}.py`.

| Script | What it tests |
| ------ | ------------- |
| `ragas_aspect_critic.py` | Evaluates an LLM response using the Ragas framework and the AspectCritic metric. The current implementation translates an English input into French and then scores the translation quality. |

### Adding new examples

Create a new file matching the naming pattern above, implement your logic, and make it executable with `python ./evals_example/<framework>_<metric>.py`.
